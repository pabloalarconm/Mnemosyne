
# Octuber 2021

### The idea for the thesis was set as: Semantics applied to mapping between diverse health record standards:


The promise of personalized medicine and precision medicine requires massive multi-modal analysis of detailed bulk health data at the international population level, in order to build accurate predictive models properly stratified to different sub-populations.  On the other end of the scale, for an individual patient, the specific details of their health and wellness situation must be available in order to properly use these predictive models. Moreover, European health data regulations are increasingly requiring that the patient themselves be given access to all of these data.  Contrast these requirements with the de facto situation where every patient’s information is dispersed over multiple locations (pharmacies, multiple clinic, multiple hospitals, patient registries, biobanks, and their personal wearable devices), where each location has its own security measures, its own data systems, its own data formats, and even its own formal disease coding systems (if any!).  These massively conflicting scenarios make the problem of health record harmonization and consolidation fertile ground for the application of semantics and other Artificial Intelligence technologies.  

The appearance of the FAIR Principles (Findable, Accessible, Interoperable and  Reusable) have provided an apparent entry-point to a novel approach to this problem.  The Principles focus on metadata, rather than data, which allows an initial emphasis on describing the nature and content of a data source, without exposing its records, thus avoiding privacy considerations. The explicit description of a data source’s content - it’s structure, its data formats, its coding standards - can therefore be used by machines to design a strategy for how to approach and interact with that data in an automated manner.  This, in turn, becomes a privacy-protecting activity, since humans never see the data itself, and machines can generate anonymous statistical profiles of the contained data, that can then be exported to machine-learning algorithms without exposing any individual patient information.

The large issue that remains, therefore, is addressing these data formats and types, and ensuring that machines correctly interpret like-with-like when extracting the same “type” of data, from different locations, in different formats, and in different coding systems, such that these large multi-modal ML models are accurate.  Similarly, when a patient explores their own data, it will be important that they can obtain a clear view of this information, without needing to understand how to interpret different formats, or map distinct coding systems onto one another.  Moreover, FAIR-based systems also provide an explicit way for the patient to express their informed consent for data usage within their medical records; parallel projects (in our lab, and others) are focused on making those consent constraints machine-interpretable, to further ensure that automated systems are respecting privacy and patient self-governance of their data.

In this thesis, we will focus on the application of semantics to the problem of mapping between medical record formats in order to get a harmonized view of the contained information.  One of the most used data model standards that facilitate data merging and (meta)data interoperability is Resource Description Framework (RDF). This framework utilizes graphs of Globally Unique Identifiers to identify data points and terminological/ontological annotations of those data points. It’s syntax is fully machine-explorable in an autonomous manner, and the graph network allows machines to accurately understand linkages between multiple data sources.

The appearance of new data transformation technologies from non-semantic data source as sheet-based formats to semantic data models as RDF data have allowed an efficient and rapid increase of this FAIR data and cutting-edge data transformation technologies are constantly appearing as feasible technologies to work with. One of these technologies is RDF Mapping Language (RML), a superset of the W3C-standardized mapping language defined to express customizing mapping rules in order to serialize RDF data based on a legacy data format/structure. RML allows a variety of data sources such as JSON, CSV and SQL among others, to transform the data using its customized mapping rules. Other representations, like YARRRML, based on YAML syntax allows the same data transformation but with a more human-readable way to express its customizing mapping rules.

Cutting-edge modeling languages have joined the cast of new semantic technologies for data. One of these technologies is LinkML. LinkML acts as a “Rosetta’s Stone” for data transformation, allowing “abstract” model specification, which can then be transformed into, for example, YAML, JSON or RDF. These models themselves can be mapped into a variety of artefacts, such as Shex for data validation, and OWL and JSON Schema among them. LinkML have participated in a variety of health registries and projects, for example, National Microbiome Data Collaborative and Simple Standard for Sharing Ontology Mappings (SSSOM).

In almost all cases, mappings between health record formats must be constructed manually, and is an extremely time-intensive and expertise-intensive exercise (to the point where it has experienced limited success to-date). While we are aware of the disappointing legacy of attempts to automate mapping between data structures (e.g. XML Schema mapping, which to our knowledge has never been entirely successful) we see differences in this case.  First, many of the data systems are, internally, well-described, and have dictionaries that could be used to provide clues as to the semantic meaning of each element.  Second, ontology mapping has, to a large degree, already been done, and can be leveraged to assist in the automated interpretation of disparate data elements.  Third, there are now several “common cataloguing” efforts - including OpenEHR and the LinkML project mentioned earlier - which are attempting to generate a comprehensive list of all data-types that can be anticipated in this space. As such, the space that needs to be explored to find likely mapping candidates is not unlimited, and becomes computationally tractable.  Finally, given the eventual use of these combined data in prediction of treatments or in clinical trials, and the likelihood of errors in a fully automated system, it would be irresponsible to leave human experts out of the process of building the mappings. As such, the most responsible aim would be to help ease and hasten the problem of building accurate mappings via decision support tools, rather than fully automated map-builders.  This, then, is the aim of this thesis - to build on all of these existing technologies and knowledge resources to design, test, and implement decision support tools that enable the rapid development of accurate health record mappings, such that we can safely and accurately and in a privacy-preserving manner, maximize the value of patient health records, and support personalized interventions to improve their treatment and health outcomes.  The testing ground will be the ongoing health record mapping initiatives within the European Joint Programme on Rare Disease (EJPRD), where representatives from a wide range of standards and formats, including OpenEHR, FIHR, OMOP are attempting to build international interoperability platforms to work together with the EJPRD and the semantic representations we have constructed for that project.


## First models based of LinkML of our CDE models are here: https://github.com/pabloalarconm/PERSEUS/tree/main/trialLinkML/linkML 